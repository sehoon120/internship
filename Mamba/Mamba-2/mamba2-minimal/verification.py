import torch
import torch.nn.functional as F
import numpy as np
from einops import rearrange
import math


def exp_fast8(x: torch.Tensor) -> torch.Tensor:
    """
    8-segment PWL ê·¼ì‚¬ë¡œ e^x ê³„ì‚°.
    e^x = 2^(x*log2(e)) = 2^(k+f) = 2^k * 2^f
    2^fëŠ” fâˆˆ[0,1)ì—ì„œ 8êµ¬ê°„(pwl) 1ì°¨ ê·¼ì‚¬.
    """
    # ìƒìˆ˜/ì¤€ë¹„
    log2e = 1.4426950408889634  # 1/ln(2)
    device, dtype = x.device, x.dtype

    # t = x*log2(e) = k + f
    t = x * torch.tensor(log2e, dtype=dtype, device=device)
    k = torch.floor(t)                          # ì •ìˆ˜ë¶€ (float)
    f = t - k                                   # ì†Œìˆ˜ë¶€ âˆˆ [0,1)

    # 8 êµ¬ê°„ ì¸ë±ìŠ¤ì™€ ì¢Œì¸¡ ê²½ê³„ f0
    seg = torch.clamp((f * 8).to(torch.int64), 0, 7)
    f0  = seg.to(dtype) / 8.0

    # êµ¬ê°„ ëì  ê°’: y0=2^(f0), y1=2^(f0+1/8)
    # (í…Œì´ë¸” ì‚¬ì „ê³„ì‚°)
    with torch.no_grad():
        boundaries = torch.arange(9, device=device, dtype=dtype) / 8.0  # 0,1/8,...,1
        pow2_table = torch.pow(torch.tensor(2.0, dtype=dtype, device=device), boundaries)
    y0 = pow2_table[seg]              # 2^(seg/8)
    y1 = pow2_table[seg + 1]          # 2^((seg+1)/8)

    # ì„ í˜• ê·¼ì‚¬: 2^f â‰ˆ y0 + slope*(f - f0)
    # slope = (y1 - y0) / (1/8) = 8*(y1 - y0)
    slope = 8.0 * (y1 - y0)
    two_pow_f = y0 + slope * (f - f0)

    # ìµœì¢…: 2^k * 2^f  (ldexp: mantissa * 2^exponent)
    # këŠ” floatì´ë¯€ë¡œ ì •ìˆ˜ë¡œ ë³€í™˜
    k_int = k.to(torch.int32)
    y = torch.ldexp(two_pow_f, k_int)
    return y


def save_tensor_as_hex(tensor, path):
    tensor = tensor.to(torch.float16).contiguous()
    with open(path, 'w') as f:
        for val in tensor.view(-1):
            u16 = val.view(torch.uint16).item()
            f.write(f"{u16:04x}\n")

def load_hex_tensor(filepath, shape):
    with open(filepath, 'r') as f:
        lines = f.readlines()
    data = [int(line.strip(), 16) for line in lines]
    arr = np.array(data, dtype=np.uint16).view(np.float16)
    return torch.tensor(arr, dtype=torch.float16).reshape(shape)

def print_tensor_fp16_hex_inline(tensor):
    tensor = tensor.to(torch.float16).contiguous()
    flat = tensor.view(-1)
    for val in flat:
        u16 = val.view(torch.uint16).item()
        print(f"{u16:04x}", end=' ')  # ì¤„ë°”ê¿ˆ ì—†ì´ ê³µë°± êµ¬ë¶„
    print('\n')  # ë§ˆì§€ë§‰ ì¤„ë°”ê¿ˆ


# B_ = 1
# H_ = 24
# P_ = 4
# N_ = 32

B_ = 1
H_ = 24
P_ = 64
N_ = 128

h_slice = 1
p_slice = 1
n_slice = 128  # 128  # ì´ ì¶•ìœ¼ë¡œëŠ” slice ë¶ˆê°€

# ê²½ë¡œ ì§€ì •
base_path = "C:/Internship/internship/Mamba/Mamba-2/mamba2-minimal/verilog/intermediate_datas"  # "C:/Internship/intermediate_datas"
dt_bias = load_hex_tensor(f"{base_path}/0_dt_bias_full_SSM.hex", (B_, H_))
dt = load_hex_tensor(f"{base_path}/0_dt_full_SSM.hex", (B_, H_))
x = load_hex_tensor(f"{base_path}/0_x_full_SSM.hex", (B_, H_, P_))
A = load_hex_tensor(f"{base_path}/0_A_full_SSM.hex", (H_,))
B = load_hex_tensor(f"{base_path}/0_B_full_SSM.hex", (B_, N_))
C = load_hex_tensor(f"{base_path}/0_C_full_SSM.hex", (B_, N_))
D = load_hex_tensor(f"{base_path}/0_D_full_SSM.hex", (H_,))
h_prev = load_hex_tensor(f"{base_path}/0_ssm_state_full_SSM.hex", (B_, H_, P_, N_))

Y = torch.zeros((B_, H_, P_), dtype=torch.float16)
ln2 = math.log(2)
    
for h_idx in range(0, H_, h_slice):
    for p_idx in range(0, P_, p_slice):
        for n_idx in range(0, N_, n_slice):
            dt_bias_tile = dt_bias[:, h_idx:h_idx+h_slice]
            dt_tile = dt[:, h_idx:h_idx+h_slice]
            x_tile = x[:, h_idx:h_idx+h_slice, p_idx:p_idx+p_slice]
            A_tile = A[h_idx:h_idx+h_slice]
            B_tile = B[:, n_idx:n_idx+n_slice]
            C_tile = C[:, n_idx:n_idx+n_slice]
            D_tile = D[h_idx:h_idx+h_slice]
            h_tile = h_prev[:, h_idx:h_idx+h_slice, p_idx:p_idx+p_slice, n_idx:n_idx+n_slice]
            # print_tensor_fp16_hex_inline(h_tile)
            # tile tensorë¡œ ë³€ê²½í•´ì„œ ì—°ì‚°
            # print((dt_tile + dt_bias_tile).shape)
            dt_sp_tile = torch.where((dt_tile + dt_bias_tile) == 0, ln2, (dt_tile + dt_bias_tile) / (1 - exp_fast8(-(dt_tile + dt_bias_tile)/ln2)))
            # dt_sp_tile = F.softplus(dt_tile + dt_bias_tile)
            dA_tile = torch.exp(dt_sp_tile * A_tile)
            dx_tile = torch.einsum("bh, bhp -> bhp", dt_sp_tile, x_tile)
            dxB_tile = torch.einsum("bhp, bn -> bhpn", dx_tile, B_tile)
            # dBx_tile = torch.einsum("bh, bn, bhp -> bhpn", dt_tile, B_tile, x_tile)
            # save_tensor_as_hex(dBx_tile, f"{base_path}/0_dBx_python.hex")
            # print('dx: ')
            # print_tensor_fp16_hex_inline(dx_tile)
            
            h_new = h_tile * rearrange(dA_tile, "b h -> b h 1 1") + dxB_tile
            # save_tensor_as_hex(h_new, f"{base_path}/0_h_new_python.hex")
            # print('h_new: ')
            # print_tensor_fp16_hex_inline(h_new)

            y = torch.einsum("bhpn, bn -> bhp", h_new, C_tile)
            # if h_idx == 0 and p_idx == 0:
                # save_tensor_as_hex(y, f"{base_path}/0_hc_python.hex")
            # print('y: ')
            # print_tensor_fp16_hex_inline(y)

            y = y + rearrange(D_tile, "h -> h 1") * x_tile
            # print_tensor_fp16_hex_inline(y)
            

            Y[:, h_idx:h_idx+h_slice, p_idx:p_idx+p_slice] += y

Y = rearrange(Y, "b h p -> b (h p)")

# ê²°ê³¼ ì €ì¥
save_tensor_as_hex(Y, f"{base_path}/0_y_out_python_full_SSM_approx.hex")
# print("(âÂ´â—¡`â) ì—°ì‚° ì™„ë£Œ! ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:", f"{base_path}/0_y_out_python.hex")
# # print("y_Python =\n", Y.view(H_, P_))
# y_out = load_hex_tensor(f"{base_path}/0_y_out.hex", (B_, H_, P_))
# # print("\ny_Hardware =\n", y_out.view(H_, P_))



def load_fp16_hex(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()
    data = [int(line.strip(), 16) for line in lines]
    return torch.tensor(np.array(data, dtype=np.uint16).view(np.float16), dtype=torch.float16)

def compare_fp16_hex(file1, file2):
    t1 = load_fp16_hex(file1)
    t2 = load_fp16_hex(file2)

    if t1.shape != t2.shape:
        raise ValueError(f"íŒŒì¼ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {t1.shape} vs {t2.shape}")

    abs_diff = torch.abs(t1 - t2)
    max_error = abs_diff.max().item()
    mean_error = abs_diff.mean().item()
    rms_error = torch.sqrt(torch.mean((t1 - t2) ** 2)).item()
    num_errors = (abs_diff > 1e-3).sum().item()  # 0.001 ì´ìƒ ì°¨ì´ ë‚˜ëŠ” í•­ëª© ìˆ˜

    print(f"ğŸ” Total elements: {t1.numel()}")
    print(f"ğŸ“Š Max abs error : {max_error}")
    print(f"ğŸ“Š Mean abs error: {mean_error}")
    print(f"ğŸ“ RMS error: {rms_error}")
    print(f"âš ï¸  Elements with >1e-3 error: {num_errors}")

    # ì°¨ì´ê°€ ë‚˜ëŠ” ì¸ë±ìŠ¤ ì¶œë ¥ (ìƒìœ„ 10ê°œ)
    topk = torch.topk(abs_diff, k=10)
    print("\nTop 10 max error entries:")
    for idx, val in zip(topk.indices, topk.values):
        i = idx.item()
        print(f"[{i}] Py={t1[i].item():.6f}, Verilog={t2[i].item():.6f}, AbsErr={val.item():.6f}")

# ì‚¬ìš© ì˜ˆì‹œ
file_pth = "C:/Internship/internship/Mamba/Mamba-2/mamba2-minimal/verilog/intermediate_datas"
file_py = f"{file_pth}/0_y_out_python_full_SSM_approx.hex"  # 0_y_out_python_full_SSM.hex"
file_v =  f"{file_pth}/0_y_out_python_full_SSM.hex"  # 0_y_out_full_SSM.hex"
compare_fp16_hex(file_py, file_v)